{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "## Testing different models, tuning their parameters and comparing performance.\n",
    "\n",
    "This is a binary classification problem, so I will consider Logistic Regression, LDA, QDA, and K-NN models. I will also evaluate the use of regularisation terms.\n",
    "\n",
    "We are building a model to predict heart disease based on a number of indicators, thus **False Negatives are BAD**! We don't want to tell someone they probably don't have heart disease when they do!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Metrics to evaluate the model\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to generate performance metrics for each model\n",
    "\n",
    "def generate_metrics(actual, predicted):\n",
    "    print(classification_report(actual, predicted))\n",
    "    \n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "\n",
    "    plt.figure(figsize = (8, 5))\n",
    "\n",
    "    sns.heatmap(cm, annot = True, fmt = '.2f', xticklabels = ['No Heart Disease', 'Heart Disease'], yticklabels = ['No Heart Disease', 'Heart Disease'])\n",
    "\n",
    "    plt.ylabel('Actual')\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_df = pd.read_csv('../data/preprocessed/heart_preprocessed.csv')\n",
    "\n",
    "X = LR_df.drop(columns=['target'])\n",
    "y = LR_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(type(X_train), type(y_train))\n",
    "\n",
    "LR_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "LR_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LR Model Analysis\n",
    "Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predicted = LR_model.predict(X_train)\n",
    "generate_metrics(y_train, y_train_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted = LR_model.predict(X_test)\n",
    "generate_metrics(y_test, y_test_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics show 84% recall on test data - pretty good for a first attempt! But let's see if we can do better.\n",
    "\n",
    "#### Parameter Analysis \n",
    "\n",
    "Let's see what parameters have the biggest impact on a heart disease prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model_coeffs = np.exp(LR_model.coef_)\n",
    "\n",
    "pd.DataFrame(LR_model_coeffs, columns=X_train.columns).T.sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<thal_2> is a big predictor of heart disease. I have previously noted that is the feature with the highest VIF! Perhaps it should be removed from the data.\n",
    "\n",
    "#### Precision-Recall Curve for Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute probability of heart disease yes/no for the training data\n",
    "LR_y_scores = LR_model.predict_proba(X_train)\n",
    "\n",
    "print(LR_model.classes_)\n",
    "print('Heart Disease: \\n      No ----- Yes')\n",
    "print(LR_y_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_precisions, LR_recalls, LR_thresholds = precision_recall_curve(\n",
    "    y_train, LR_y_scores[:,-1]\n",
    ")\n",
    "\n",
    "# Plot values of precisions, recalls, and thresholds\n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "plt.plot(LR_thresholds, LR_precisions[:-1], 'b--', label = 'precision')\n",
    "\n",
    "plt.plot(LR_thresholds, LR_recalls[:-1], 'g--', label = 'recall')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "\n",
    "plt.legend(loc = 'upper left')\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows recall drops off quickly after about 0.6 threshold. Because recall is critical, lets lower the threshold to .3 to see if we can increase recall without compromising precision too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_3_threshold = .3\n",
    "\n",
    "# adjusted decisions using new 0.3 threshold\n",
    "# maps to a new boolean arr where if P(Heart Disease = true) > .3 -> it maps to true, otherwise false.\n",
    "y_train_predicted_point_3 = LR_y_scores[:,1] > point_3_threshold\n",
    "generate_metrics(y_train, y_train_predicted_point_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected - false negative recall improved, and not a terrible false positive recall either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
